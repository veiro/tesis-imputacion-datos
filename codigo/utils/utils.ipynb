{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": [
        "nWJC1OoQ8ZBh",
        "J7fRO4vrDHB9"
      ],
      "authorship_tag": "ABX9TyORwHliZTBaRUqZIJyyi2pQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veiro/tesis-imputacion-datos/blob/main/codigo/utils/utils.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install \"pandas<2.0.0"
      ],
      "metadata": {
        "id": "Udj55tLvj2fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5685f0f5-7f14-4805-f07d-2c534dc05dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n",
            "/bin/bash: -c: line 2: syntax error: unexpected end of file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "e4x-9rq9wzbq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffFfec6SwoCY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "from tqdm import tqdm\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Version\n"
      ],
      "metadata": {
        "id": "d9JASWCBxRrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def version():\n",
        "  return 30"
      ],
      "metadata": {
        "id": "MdB4oEdsoNu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mostrar datos\n"
      ],
      "metadata": {
        "id": "nWJC1OoQ8ZBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mostrarDatos(df_data, nombreDatos=''):\n",
        "  '''\n",
        "  print(\"DESCRIBE:\\n\")\n",
        "  print(df_data.describe())\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
        "  print(\"INFO:\\n\")\n",
        "  print(df_data.info())\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
        " '''\n",
        "  print(\"Nombre datos: \"+ nombreDatos)\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
        "  print(df_data.columns)\n",
        "\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "  print(\"SHAPE:\\n\")\n",
        "  print(df_data.shape)\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "  print(\"Count of NaN:\\n\")\n",
        "  print(df_data.isna().sum().sum())\n",
        "  print(\"\\n--------------------------------------------------------------------------------\\n\")"
      ],
      "metadata": {
        "id": "3JBAUpQS8aru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encode-decode\n"
      ],
      "metadata": {
        "id": "EXE7xGmAxUzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# variables gloables\n",
        "encoded_df_Country_Region_cols = 0\n",
        "encoderOHE = OneHotEncoder(handle_unknown='ignore')\n",
        "encoderLE1 = LabelEncoder()\n",
        "encoderLE2 = LabelEncoder()\n",
        "encoderLE3 = LabelEncoder()\n",
        "\n",
        "def save_pickle_model(name, model, PATH_DATA_PROCESADA):\n",
        " with open(PATH_DATA_PROCESADA + '/' + name , 'wb') as handle:\n",
        "    pickle.dump(model, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def load_pickle_model(name, PATH_DATA_PROCESADA):\n",
        "  with open(PATH_DATA_PROCESADA + '/' + name, 'rb') as handle:\n",
        "      model = pickle.load(handle)\n",
        "  return model\n",
        "\n",
        "def encoding(dataset, PATH_DATA_PROCESADA):\n",
        "  global encoderOHE\n",
        "  global encoderLE1\n",
        "  global encoderLE2\n",
        "  global encoderLE3\n",
        "  global encoded_df_Country_Region_cols\n",
        "\n",
        "  # ENCODING\n",
        "  # 1- onehotenconding de Country_Region\n",
        "  df=dataset\n",
        "\n",
        "  encoded_data = encoderOHE.fit_transform( df[['Country_Region']])\n",
        "  encoded_df_Country_Region = pandas.DataFrame(encoded_data, columns=encoderOHE.get_feature_names_out(['Country_Region']))\n",
        "  encoded_df_Country_Region_cols = encoded_df_Country_Region.shape[1]\n",
        "\n",
        "  df = df.drop(['Country_Region'], axis=1)\n",
        "\n",
        "  # hice este concat horrible por que por alguna razon no me andaba bien el concant ordinario\n",
        "  pos = encoded_df_Country_Region_cols\n",
        "  for col in df.columns:\n",
        "    encoded_df_Country_Region.insert( pos, col, df[col].to_numpy(), True)\n",
        "    pos=pos+1\n",
        "\n",
        "  df = encoded_df_Country_Region\n",
        "\n",
        "  # 2 Province_State, Last_Update, date\n",
        "  df['Province_State_encoded'] = encoderLE1.fit_transform(df['Province_State'])\n",
        "  df = df.drop(['Province_State'], axis=1)\n",
        "\n",
        "  df['Last_Update_encoded'] = encoderLE2.fit_transform(df['Last_Update'])\n",
        "  df = df.drop(['Last_Update'], axis=1)\n",
        "\n",
        "  df['date_encoded'] = encoderLE3.fit_transform(df['date'])\n",
        "  df = df.drop(['date'], axis=1)\n",
        "\n",
        "  df.reset_index(drop=True)\n",
        "\n",
        "  # guardo los modelos utilizados:\n",
        "  #save_pickle_model('encoderOHE',encoderOHE, PATH_DATA_PROCESADA )\n",
        "  #save_pickle_model('encoderLE1',encoderLE1, PATH_DATA_PROCESADA)\n",
        "  #save_pickle_model('encoderLE2',encoderLE1, PATH_DATA_PROCESADA)\n",
        "  #save_pickle_model('encoderLE3',encoderLE1, PATH_DATA_PROCESADA)\n",
        "  #save_pickle_model('encoded_df_Country_Region_cols',encoderLE1, PATH_DATA_PROCESADA)\n",
        "  return df\n",
        "\n",
        "# paso los datos imputados y los datos orignales transformados para hacer validaciones de formato y arreglar incosistencias por la imputacion.\n",
        "def decode(imputed_data, data_encoded, PATH_DATA_PROCESADA):\n",
        "\n",
        "  encoderOHE = load_pickle_model('encoderOHE', PATH_DATA_PROCESADA)\n",
        "  encoderLE1 = load_pickle_model('encoderLE1', PATH_DATA_PROCESADA)\n",
        "  encoderLE2 = load_pickle_model('encoderLE2', PATH_DATA_PROCESADA)\n",
        "  encoderLE3 = load_pickle_model('encoderLE3', PATH_DATA_PROCESADA)\n",
        "  encoded_df_Country_Region_cols = load_pickle_model('encoded_df_Country_Region_cols', PATH_DATA_PROCESADA)\n",
        "\n",
        "  # esta linea la agregue para evitar problemas en data_encoded\n",
        "  data_transformed = data_encoded.copy()\n",
        "  df = pandas.DataFrame(imputed_data, columns = data_transformed.columns)\n",
        "  # DECODE\n",
        "  # 1- onehotenconding de Country_Region.\n",
        "  encoded_df_Country_Region = df.iloc[:,  :encoded_df_Country_Region_cols]\n",
        "\n",
        "  df['Country_Region'] = encoderOHE.inverse_transform(encoded_df_Country_Region).reshape(-1)\n",
        "  df.drop(columns=df.columns[:encoded_df_Country_Region_cols], axis=1,  inplace=True)\n",
        "\n",
        "\n",
        "  #2- Province_State, Last_Update, date\n",
        "\n",
        "  Province_State_encoded_count = data_transformed.Province_State_encoded.unique().tolist().count(1)\n",
        "  Last_Update_encoded_count = data_transformed.Last_Update_encoded.unique().tolist().count(1)\n",
        "  date_encoded_encoded_count = data_transformed.date_encoded.unique().tolist().count(1)\n",
        "\n",
        "  Province_State_encoded_most_freq = df['Province_State_encoded'].value_counts().idxmax()\n",
        "  Last_Update_encoded_most_freq = df['Last_Update_encoded'].value_counts().idxmax()\n",
        "  date_encoded_most_freq = df['date_encoded'].value_counts().idxmax()\n",
        "  Country_Region_most_freq= df['Country_Region'].value_counts().idxmax()\n",
        "\n",
        "  # esto es por si el metodo asigno una valor real, pero las codificaciones validacioenes son enterasm\n",
        "  df['Province_State_encoded'] = df['Province_State_encoded'].astype(int)\n",
        "  df['date_encoded'] = df['date_encoded'].astype(int)\n",
        "  df['Last_Update_encoded'] = df['Last_Update_encoded'].astype(int)\n",
        "\n",
        "  for i,  row in tqdm(df.iterrows()):\n",
        "    # en algunos casos inventa valores que no existen, entonces como es un error le asigno el primero pero puede ser cualquier valor.\n",
        "\n",
        "    if(row['Province_State_encoded'] < 1 or row['Province_State_encoded'] > Province_State_encoded_count ):\n",
        "      #significa que genero algo que no existe, pongo el mas frequente\n",
        "      df.at[i,'Province_State_encoded'] = Province_State_encoded_most_freq\n",
        "\n",
        "\n",
        "    if(row['Last_Update_encoded'] <1 or row['Last_Update_encoded'] > Last_Update_encoded_count ):\n",
        "      #significa que genero algo que no existe, pongo el mas frequente\n",
        "      df.at[i,'Last_Update_encoded'] = Last_Update_encoded_most_freq\n",
        "\n",
        "\n",
        "    if(row['date_encoded'] <1 or row['date_encoded'] > date_encoded_encoded_count):\n",
        "      #significa que genero algo que no existe, pongo el mas frequente\n",
        "      df.at[i,'date_encoded'] = date_encoded_most_freq\n",
        "\n",
        "    if(row['Country_Region'] is None):\n",
        "        # fallo al predecir el valor para alguna columna del onehotencoding, si no hago nada agrega nan\n",
        "        # voy a poner el mas frecuente en este caso\n",
        "        df.at[i,'Country_Region'] = Country_Region_most_freq\n",
        "\n",
        "\n",
        "  df['Province_State'] = encoderLE1.inverse_transform(df['Province_State_encoded'].astype(int) )\n",
        "  df['Last_Update'] = encoderLE2.inverse_transform(df['Last_Update_encoded'].astype(int) )\n",
        "  df['date'] = encoderLE3.inverse_transform(df['date_encoded'].astype(int) )\n",
        "\n",
        "\n",
        "\n",
        "  df = df.drop(['Province_State_encoded'], axis=1)\n",
        "  df = df.drop(['Last_Update_encoded'], axis=1)\n",
        "  df = df.drop(['date_encoded'], axis=1)\n",
        "\n",
        "  #3- ordeno columnas\n",
        "\n",
        "  col = df.pop('Province_State')\n",
        "  df.insert(0, col.name, col)\n",
        "\n",
        "  col = df.pop('Country_Region')\n",
        "  df.insert(1,  col.name, col)\n",
        "\n",
        "  col = df.pop('Last_Update')\n",
        "  df.insert(2,  col.name, col)\n",
        "\n",
        "  # cambio tipo de dato\n",
        "\n",
        "  df['Confirmed'] = df['Confirmed'].astype(int)\n",
        "\n",
        "  df['Deaths'] = df['Deaths'].astype(int)\n",
        "\n",
        "\n",
        "  # cantidad de cifras significativas>\n",
        "\n",
        "  df[\"Lat\"] = round(df['Lat'],6)\n",
        "  df[\"Long_\"] = round(df['Long_'],4)\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "Use3-RLExbzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtener datos"
      ],
      "metadata": {
        "id": "J7fRO4vrDHB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtenerDatosTrain(MODO_DESARROLLO, PATH_DATA_PROCESADA):\n",
        "  train_data_encoded_open = pandas.read_csv(PATH_DATA_PROCESADA + '/csse_covid_19_data_train_encoded=True_missing=False_' + 'Desarrollo=False' + '.csv')\n",
        "  train_data_encoded_missing_open = pandas.read_csv(PATH_DATA_PROCESADA + '/csse_covid_19_data_train_encoded=True_missing=True_' + 'Desarrollo=False'  + '.csv')\n",
        "  mask_train_open = np.load(PATH_DATA_PROCESADA + '/csse_covid_19_data_train_mask_'+ 'Desarrollo=False'+ '.npy')\n",
        "\n",
        "  if(MODO_DESARROLLO):\n",
        "    train_data_encoded_open = train_data_encoded_open[:10000]\n",
        "    train_data_encoded_missing_open = train_data_encoded_missing_open[:10000]\n",
        "    mask_train_open = mask_train_open[:10000]\n",
        "\n",
        "  return train_data_encoded_open, train_data_encoded_missing_open, mask_train_open.astype(int)\n"
      ],
      "metadata": {
        "id": "NP7KwWYha9Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def obtenerDatosTest(MODO_DESARROLLO, PATH_DATA_PROCESADA):\n",
        "  test_data_encoded_open = pandas.read_csv(PATH_DATA_PROCESADA + '/csse_covid_19_data_test_encoded=True_missing=False_' + 'Desarrollo=False' + '.csv').to_numpy()\n",
        "  test_data_encoded_missing_open = pandas.read_csv(PATH_DATA_PROCESADA + '/csse_covid_19_data_test_encoded=True_missing=True_' + 'Desarrollo=False'  + '.csv').to_numpy()\n",
        "  mask_test_open = np.load(PATH_DATA_PROCESADA + '/csse_covid_19_data_test_mask_'+ 'Desarrollo=False'+ '.npy')\n",
        "\n",
        "  if(MODO_DESARROLLO):\n",
        "    test_data_encoded_open = test_data_encoded_open[:10000]\n",
        "    test_data_encoded_missing_open = test_data_encoded_missing_open[:10000]\n",
        "    mask_test_open = mask_test_open[:10000]\n",
        "\n",
        "  return test_data_encoded_open, test_data_encoded_missing_open, mask_test_open.astype(int)"
      ],
      "metadata": {
        "id": "bAeHKz-bbUKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embbedinggs"
      ],
      "metadata": {
        "id": "B-VflhKXI3OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se baso en https://arxiv.org/pdf/1604.06737.pdf\n",
        "y yo en  https://colab.research.google.com/drive/13U4YRIdEu7SWS1ttiJPSrSayQHNbIaT_?usp=sharing#scrollTo=qSoepAKnfmvN"
      ],
      "metadata": {
        "id": "0RMRL5B5q4p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "fpzFalW0q0MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "def getEmbbeding(X_test, PATH_DATA_PROCESADA):\n",
        "  X_test_emb = X_test.copy()\n",
        "  path_embbedings=PATH_DATA_PROCESADA + '/embbedings_weights_' + '.pkl'\n",
        "  #with open(path_embbedings , 'rb') as f:\n",
        "      #col_cat_emb_open = pickle.load(f)\n",
        "  col_cat_emb_open = pd.read_pickle(path_embbedings)\n",
        "  for col  in col_cat_emb_open:\n",
        "    X_test_emb = pd.merge(X_test_emb, col.get(\"cat_emb\"), left_on=col.get(\"columna\"), right_on=col.get(\"columna\"), how='inner')\n",
        "    #borro columnas que por ahora no uso, si quiero hacer la operacion inversa las necesito.\n",
        "\n",
        "  return X_test_emb"
      ],
      "metadata": {
        "id": "d8-s2FcnVC2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metricas"
      ],
      "metadata": {
        "id": "i3M312jA--wS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RMSE"
      ],
      "metadata": {
        "id": "oHpHOt-EDMDC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "from math import sqrt\n",
        "def rmse(ori_data, imputed_data, data_m):\n",
        "  return np.sqrt(np.mean((imputed_data * data_m - ori_data * data_m)**2))\n",
        "\n"
      ],
      "metadata": {
        "id": "2tGwKntbk9pa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## similaridad de coseno"
      ],
      "metadata": {
        "id": "9ic5f53HYn_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "def calcular_similitud_cosine_promedio(df1, df2):\n",
        "    similitudes = []\n",
        "    for index in range(df1.shape[1]):\n",
        "        tupla_df1 = df1.loc[index].values.reshape(1, -1)\n",
        "        tupla_df2 = df2.loc[index].values.reshape(1, -1)\n",
        "        similitud = cosine_similarity(tupla_df1, tupla_df2)\n",
        "        similitudes.append(similitud[0][0])\n",
        "    similitud_promedio = np.mean(similitudes)\n",
        "    return similitud_promedio"
      ],
      "metadata": {
        "id": "NjKJQUigW83C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# obtener df imputados\n"
      ],
      "metadata": {
        "id": "qTtTS45ZDkdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def obtener_df_imputados(ori_data, imputed_data, data_m):\n",
        "  # data_m = 1 si era null o 0 si no era null\n",
        "  ori_data_np = ori_data.to_numpy()\n",
        "  imputed_data_np  = imputed_data.to_numpy()\n",
        "\n",
        "  imputed_data_np = imputed_data_np * data_m + ori_data_np * (1- data_m)\n",
        "\n",
        "  df1 = pd.DataFrame(ori_data_np, columns = ori_data.columns)\n",
        "  df2 = pd.DataFrame(imputed_data_np, columns = ori_data.columns)\n",
        "  return df1, df2\n"
      ],
      "metadata": {
        "id": "-UiapMvyDo58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# prettyPrint"
      ],
      "metadata": {
        "id": "RwJffgumh5b9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prettyPrintName(strat):\n",
        "  if('Transformer' in  strat):\n",
        "    return 'Transformer'\n",
        "  elif('XGBoost' in  strat):\n",
        "    return 'XGBoost'\n",
        "  elif('Knn' in  strat):\n",
        "    return 'Knn'\n",
        "  elif('GAIN' in  strat):\n",
        "    return 'GAIN'\n",
        "  elif('MIDA' in  strat):\n",
        "    return 'MIDASpy'\n",
        "  elif('constant' in  strat):\n",
        "    return 'constant'\n",
        "  elif('most_frequent' in  strat):\n",
        "    return 'most_frequent'\n",
        "  elif('mean' in  strat):\n",
        "    return 'mean'\n",
        "  elif('median' in  strat):\n",
        "    return 'median'\n",
        "  elif('IterativeImputer' in  strat):\n",
        "    return 'IterativeImputer'\n",
        "  else:\n",
        "    return strat"
      ],
      "metadata": {
        "id": "Ii9p1kVLh2-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}